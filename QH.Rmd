---
title: "Final Project"
author: "Group L"
output:
  html_document:
    toc: true
    self_contained: true
    keep_md: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message=FALSE))
```


# Overview

For this project, we explored two datasets -- tweets about Covid-19 vaccine and the reported side effects of it.
Firstly, we looked into what people are discussing when they tweet about Covid-19 vaccine. Then we narrowed it down to the side effect discussions, to see what's commonly mentioned here and where the tweets were sent.
Secondly, we focused on the adverse reactions reported from 2020-12-01 to 2021-3-31. The visualization aims to provide an insight into who are the people reporting side effects and how do they compare to the general population; the most common reported symptoms etc. We also analyzed tweets associated with the covid-19 vaccine to see people's attitudes on the vaccine.

# Data

- All COVID-19 Vaccines Tweets
https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets
- Vaccine Adverse Event Reporting System (VAERS)1 established by the Food and Drug Administration (FDA) and the Centers for Disease Control and Prevention (CDC) 
https://vaers.hhs.gov/data/datasets.html?
- Allocations of Covid-19 vaccines produced by Pfizer and Moderna into different States by CDC https://data.cdc.gov/Vaccinations/COVID-19-Vaccine-Distribution-Allocations- by-Juris/saz5-9hgg https://data.cdc.gov/Vaccinations/COVID-19-Vaccine-Distribution-Allocations- by-Juris/b7pe-5nws
- COVID-19 Vaccination Demographic Data. Vaccination by age. https://www.cdc.gov/coronavirus/2019-ncov/vaccines/distributing/demographics-vaccination-data.html
- State-by-state data on COVID-19 vaccinations in the United States 
https://ourworldindata.org/us-states-vaccinations

# Tweets about Covid-19 Vaccine

## What Do People Tweet about Covid-19 Vaccines ? 

```{r}
library(dplyr)
library(tm)
library(stopwords)
library(tidytext)
library(wordcloud)
library(tidyverse)
library(hunspell)
library(data.table)
# library(RColorBrewer)

tweet <- fread("./tweet_data_QH/vaccination_all_tweets_ansi.csv")
```


```{r}
load("./tweet_data_QH/term_stemmed_all.RData")
```

```{r eval=FALSE, include=FALSE}
# str(tt)
# all oringinal tweets

# clean text
tt <- tweet[,c("id","text")]
tt <- rename(tt, "doc_id" = 'id')
tt$text <- as.character(tt$text)
# make a corpus
library(tm)
tt_source <- DataframeSource(tt)
tt_corpus <- VCorpus(tt_source)
tt_corpus


library(stopwords)
# preprocessing function----
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, c(stopwords(source = "smart")))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
# preprocessing
tt_clean <- clean_corpus(tt_corpus)
tt_dtm <- DocumentTermMatrix(tt_clean)

library(tidytext)
tt_tidy <- tidy(tt_dtm)
# mannually remove residual unwanted words
tt_tidy <- tt_tidy %>% 
  filter(grepl("[a-z]*\\…",term) == F) %>% 
  filter(grepl("http([a-z]*[A-Z]*[0-9]*)*",term) == F) %>% 
  # filter(grepl("([A-Z]*[a-z]*)*'([A-Z]*[a-z]*)*'",term) == F) %>% # 去除
  filter(term != "the") %>% 
  filter(term != "this") %>% 
  group_by(term) %>% 
  mutate(n=n()) %>% 
  arrange(desc(n))

tt_tidy$term2 <- hunspell::hunspell_stem(tt_tidy$term,"en_US")
ttt <- tt_tidy
ttt$term2 <- lapply(ttt$term2, function(x) if(identical(x, character(0))) NA_character_ else x)
stem1 <- ttt %>% 
  ungroup() %>% 
  mutate(index = row_number())%>% 
  unnest(term2)
stem1 <- stem1 %>% mutate(term3 = ifelse(is.na(term2),term,term2))
stemmed_all <- stem1[c(1,7,3)] %>% 
  rename(term = term3) %>% 
  group_by(term) %>% 
  mutate(n = sum(count)) %>% 
  arrange(desc(n))
save(stemmed_all,file = "term_stemmed_all.RData")
```

```{r echo=FALSE, dpi= 300,out.width = '70%',out.height = '70%'}
# plot word cloud
library(wordcloud)
set.seed(377)
library(RColorBrewer)
# reds <- brewer.pal(5, "RdPu")
purple_orange <- brewer.pal(10, "RdYlBu")
stemmed_all_p <- unique(stemmed_all
                    [c(2,4)]) # dataset for plotting
# save(stemmed_p,file = "term_stemmed.RData")
wordcloud(stemmed_all_p$term,stemmed_all_p$n,
          max.words = 500, color=purple_orange, scale=c(3,0.5),min.freq = 600)
```

## A Co-occurrence Network of Keywords

```{r include=FALSE}
library(tidyverse)
library(tidyr)
library(rlist)
library(igraph)
library(ggnetwork)
library(networkD3)
```

```{r eval=FALSE, include=FALSE}
# make an co-occurence matrix
list1k <- stemmed_all[,c(2,4)] %>% unique() %>% arrange(desc(n)) %>% head(1000) #n>=67
top1k <- stemmed_all %>% filter(term %in% list1k$term)
words <- top1k[,c(2,1)]

cooc <- merge(words,words,by='document')
cooc <- cooc %>%
  filter(term.x != term.y) %>% # remove one's relation with it self
  count(term.x,term.y) # times of co-ocurrence
cooc <- cooc %>% arrange(desc(n))
# remove replecate records (undirected edges)
cooc <- cooc %>% rename(from=term.x) %>% rename(to=term.y)
cooc2 <- cooc %>% filter(n>=100)
library(tidyverse)
dedup <- cooc2 %>%
  mutate(normalized = map2_chr(from, to, ~paste(sort(c(.x, .y)),
                                                collapse = ""))) %>%
  group_by(normalized,n) %>%
  summarise(from = first(from),
            to = first(to)) %>%
  ungroup() %>%
  select(-normalized) %>%
  arrange(desc(n))
uniedge <- dedup[,c(2,3,1)]
# save(cooc, file = "coocedge.Rdata")
# save(uniedge, file = "uniquedges.Rdata")
# library(readr)
# write_csv(cooc, path = "coocedge.csv")
```

```{r}
load("./tweet_data_QH/coocedge.Rdata")
load("./tweet_data_QH/uniquedges.Rdata")
```


```{r include=FALSE}
# make a graph----
edges <- as.matrix(uniedge)
g <- graph_from_data_frame(edges, directed = F, vertices = NULL)
# g
# calculate indegree and outdegree (centrality)
V(g)$deg <- degree(g, mode = "total", loops = F, normalized = FALSE)
# add weight(n) as edge width
E(g)$width <- E(g)$n
# as_data_frame(g, what="edges") 
# g
# plot(g)
```

```{r include=FALSE}
# prepare data for plotting----
library(ggnetwork)
set.seed(12)
# class(g)
# summary(g)
df <- ggnetwork(g,layout = with_kk())
# df
att <- rename(unique(stemmed_all[,c(2,4)]),name = term)
df <- left_join(df,att,by="name")
df$width <- as.numeric(df$width)/100

#find clusters
wc <- cluster_walktrap(g,weights = E(g)$width)  # find "communities"
# class(c)
# sizes(wc)
members <- membership(wc)
mm <- as.data.frame(cbind(names(members),members)) %>% rename(.,name=V1) %>% rename(.,cluster=members)
# ignore tiny groups
mm_b <- mm %>% group_by(cluster) %>% 
  mutate(n = n()) %>% 
  mutate(group = ifelse(n>=5,n,1))
# summary(as.factor(mm_b$group))
# name the groups
mm_b$group <- as.factor(mm_b$group) # numeric to factor
mm_b$group <- ordered(mm_b$group, levels = c(1,7,23,37,212), labels = c("undefined","group1","group2","group3","group4"))
mm_b$group <- as.character(mm_b$group)
df_m <- merge(mm_b,df,by='name')

# # plot the clusters
# cp <- unique(df_m[,c(1,4)])
# library(ggthemes)
# cp %>% ggplot(aes(name,group)) +
#   geom_jitter(height = 0.2) +
#   theme_clean() +
#   labs(title = "Automatically Detected Clusters",
#        tag = "Figure 2",
#        y = "cluster") +
#   theme(axis.text.x = element_blank(),axis.title.x = element_blank()) +
#   scale_colour_manual(values = c("blue", "green", "red")) +
#   labs(color='Party')

```

```{r eval=FALSE, include=FALSE}
# plotting network
ggplot(df_m, aes(x, y, xend = xend, yend = yend)) +
  geom_edges(aes(size = width),alpha = 0.2,color="seashell3",curvature = 0.2) +
  geom_nodes(data=subset(df_m, group=="undefined"),aes(size=n.y), color = "gray3", alpha = 0.5) + # terms not in big clusters
  geom_nodes(data=subset(df_m, group=="group1"),aes(size=n.y), color = "darkred", alpha = 0.5) + # group1
  geom_nodes(data=subset(df_m, group=="group2"),aes(size=n.y), color = "blue4", alpha = 0.5) + # group2
  geom_nodes(data=subset(df_m, group=="group3"),aes(size=n.y), color = "purple", alpha = 0.5) + # group3
  geom_nodes(data=subset(df_m, group=="group4"),aes(size=n.y), color = "orange", alpha = 0.5) + # group4
  geom_nodetext_repel(aes(label = name),size = 2.5) + 
  theme_blank()
# no text for central nodes ---- try interactive graph

```

```{r echo=FALSE}
library(networkD3)
# Convert to object suitable for networkD3
g_d3 <- igraph_to_networkD3(g, group = members)
# Create force directed network plot
forceNetwork(Links = g_d3$links, Nodes = g_d3$nodes, 
             Source = 'source', Target = 'target', 
             NodeID = 'name', Group = 'group',
             fontSize = 50, fontFamily = "Times New Roman",
             zoom = T)
# no specifying link width (would look messy)
```

## Sentiment Analysis

# Tweets about Covid-19 Vaccine's Side Effects

## What Do They Tweet about Covid-19 Vaccine's Side Effects ? 

```{r eval=FALSE, include=FALSE}
# all tweets mentioned "side effect" or "sideeffect"
setweet <- read.csv("./tweet_data_QH/tweet_side_effect.csv")
setweet <- unique(setweet)

# clean text
tt <- setweet[,c("id","text")]
tt <- rename(tt, "doc_id" = 'id')
tt$text <- as.character(tt$text)
# make a corpus
library(tm)
tt_source <- DataframeSource(tt)
tt_corpus <- VCorpus(tt_source)
tt_corpus


library(stopwords)
# preprocessing function----
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, c(stopwords(source = "smart")))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
# preprocessing
tt_clean <- clean_corpus(tt_corpus)
tt_dtm <- DocumentTermMatrix(tt_clean)

library(tidytext)
tt_tidy <- tidy(tt_dtm)
# mannually remove residual unwanted words
tt_tidy <- tt_tidy %>% 
  filter(grepl("[a-z]*\\…",term) == F) %>% 
  filter(grepl("http([a-z]*[A-Z]*[0-9]*)*",term) == F) %>% 
  # filter(grepl("([A-Z]*[a-z]*)*'([A-Z]*[a-z]*)*'",term) == F) %>% # 去除
  filter(term != "the") %>% 
  filter(term != "this") %>% 
  group_by(term) %>% 
  mutate(n=n()) %>% 
  arrange(desc(n))

tt_tidy$term2 <- hunspell::hunspell_stem(tt_tidy$term,"en_US")
ttt <- tt_tidy
ttt$term2 <- lapply(ttt$term2, function(x) if(identical(x, character(0))) NA_character_ else x)
stem1 <- ttt %>% 
  ungroup() %>% 
  mutate(index = row_number())%>% 
  unnest(term2)
stem1 <- stem1 %>% mutate(term3 = ifelse(is.na(term2),term,term2))
stemmed <- stem1[c(1,7,3)] %>% 
  rename(term = term3) %>% 
  group_by(term) %>% 
  mutate(n = sum(count)) %>% 
  arrange(desc(n))
save(stemmed,file = "term_stemmed_side_effect.RData")
```

```{r include=FALSE}
load("term_stemmed_side_effect.RData")
```

```{r echo=FALSE, dpi= 300,out.width = '70%',out.height = '70%'}
# plot word cloud
library(wordcloud)
set.seed(377)
library(RColorBrewer)
# reds <- brewer.pal(5, "RdPu")
purple_orange <- brewer.pal(10, "RdYlBu")
stemmed_p <- unique(stemmed[c(2,4)]) # dataset for plotting
# save(stemmed_p,file = "term_stemmed.RData")
wordcloud(stemmed_p$term,stemmed_p$n,
          max.words = 500, color=purple_orange, scale=c(3.5,0.5),min.freq = 20)
```


